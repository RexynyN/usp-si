{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from faker import Faker\n",
    "from os.path import join as path_join\n",
    "from datetime import datetime as dt\n",
    "\n",
    "Faker.seed(16042002)\n",
    "faker_br = Faker('pt_BR')\n",
    "faker_en = Faker('en_IN')\n",
    "faker_jp = Faker('ja_JP')\n",
    "\n",
    "\n",
    "CHUNKSIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf21c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVRO_PATH = \"livros/treated/\"\n",
    "ARTIGO_PATH = \"artigos/treated/\"\n",
    "DVD_PATH = \"dvds/treated/\"\n",
    "REVISTA_PATH = \"revistas/treated/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01ef37",
   "metadata": {},
   "source": [
    "## Importar para o Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce029bff",
   "metadata": {},
   "source": [
    "1. Bibliotecas (OK)\n",
    "2. Usuarios (OK)\n",
    "3. Autores (OK)\n",
    "4. Livros (OK)\n",
    "5. Revistas (OK)\n",
    "6. DVD's (OK)\n",
    "7. Artigos (OK)\n",
    "8. Autorias (OK)\n",
    "9. Midia (OK)\n",
    "10. Emprestimos \n",
    "11. Penalização "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Conexão com o banco de dados PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"onixlibrary\",\n",
    "    user=\"super_user\",\n",
    "    password=\"carimboatrasado\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Função para inserir dados no banco\n",
    "def insert_data(table_name, data, columns):\n",
    "    query = f\"INSERT INTO {table_name} ({', '.join(columns)}) VALUES %s\"\n",
    "    execute_values(cursor, query, data)\n",
    "    conn.commit()\n",
    "\n",
    "def generate_user():\n",
    "    user = {}\n",
    "    user['name'] = faker_br.name()\n",
    "    user['email'] = faker_br.email()\n",
    "    user['address'] = ' - '.join(faker_br.address().split('\\n'))\n",
    "    user['phone_number'] = faker_br.phone_number()\n",
    "    return user\n",
    "\n",
    "def generate_user_list():\n",
    "    user = []\n",
    "    user.append(faker_br.name())\n",
    "    user.append(faker_br.email())\n",
    "    user.append(' - '.join(faker_br.address().split('\\n')))\n",
    "    user.append(faker_br.phone_number())\n",
    "    return user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "insert_data(\"usuario\", \n",
    "            [generate_user_list() for _ in range(113_000)], \n",
    "            ['nome', 'email', 'endereco', 'telefone'])\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM usuario\", conn)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crie uma lista de bibliotecas fictícias\n",
    "fake_libraries = [\n",
    "    \"Biblioteca Onix Jaçanã\",\n",
    "    \"Biblioteca Onix Tatuapé\",\n",
    "    \"Biblioteca Onix Vila Prudente\",\n",
    "    \"Biblioteca Onix Paulista\",\n",
    "    \"Biblioteca Onix Santana\",\n",
    "    \"Biblioteca Onix Santo André\",\n",
    "    \"Biblioteca Onix São Bernardo do Campo\",\n",
    "    \"Biblioteca Onix São Caetano do Sul\",\n",
    "    \"Biblioteca Onix São José dos Campos\",\n",
    "    \"Biblioteca Onix Higienópolis\",\n",
    "    \"Biblioteca Onix Vila Madalena\",\n",
    "]\n",
    "fake_library_addresses = [\n",
    "    \"Rua das Palmeiras, 123 – Jaçanã, São Paulo – SP, 02260-000\",\n",
    "    \"Av. Álvaro Ramos, 456 – Tatuapé, São Paulo – SP, 03310-000\",\n",
    "    \"Rua José Zappi, 789 – Vila Prudente, São Paulo – SP, 03138-000\",\n",
    "    \"Av. Paulista, 1001 – Bela Vista, São Paulo – SP, 01311-100\",\n",
    "    \"Rua Voluntários da Pátria, 321 – Santana, São Paulo – SP, 02010-000\",\n",
    "    \"Rua General Glicério, 159 – Centro, Santo André – SP, 09015-330\",\n",
    "    \"Av. Faria Lima, 987 – Centro, São Bernardo do Campo – SP, 09710-000\",\n",
    "    \"Rua Alegre, 202 – Santa Paula, São Caetano do Sul – SP, 09560-300\",\n",
    "    \"Av. Adhemar de Barros, 1550 – Jardim São Dimas, São José dos Campos – SP, 12245-010\",\n",
    "    \"Rua Itacolomi, 415 – Higienópolis, São Paulo – SP, 01239-000\",\n",
    "    \"Rua Harmonia, 678 – Vila Madalena, São Paulo – SP, 05435-001\",\n",
    "]\n",
    "insert_data(\"biblioteca\", \n",
    "            [[bib, add] for bib, add in zip(fake_libraries, fake_library_addresses)], \n",
    "            ['nome', 'endereco'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"livros/authors.csv\", dtype=str, sep=\",\")\n",
    "max_id = df['author_id'].astype(int).max() + 1\n",
    "\n",
    "authors_set = df['author_name'].tolist()\n",
    "id_authors_set = df['author_id'].tolist()\n",
    "\n",
    "\n",
    "df = pd.concat([pd.read_csv(path_join(ARTIGO_PATH, file), dtype=str, sep=\",\") for file in os.listdir(ARTIGO_PATH)], ignore_index=True)[['autores']]\n",
    "df = pd.concat([df, pd.concat(\n",
    "    [pd.read_csv(path_join(REVISTA_PATH, file), dtype=str, sep=\",\") for file in os.listdir(REVISTA_PATH)], ignore_index=True)[['autores']]\n",
    "], ignore_index=True)\n",
    "\n",
    "\n",
    "authors = set()\n",
    "for idx, row in df.iterrows():\n",
    "    authors.update({ a.strip() for a in str(row['autores']).split('|')})\n",
    "\n",
    "authors = { str(k + max_id): v for k, v in enumerate(authors) if v.strip()}\n",
    "id_authors = list(authors.keys())\n",
    "names_authors = list(authors.values())\n",
    "\n",
    "id_authors.extend(id_authors_set)\n",
    "names_authors.extend(authors_set)\n",
    "\n",
    "# Crie uma lista de autores fictícios\n",
    "\n",
    "\n",
    "birth = [faker_br.date_between(start_date='-100y', end_date='-18y').strftime('%Y-%m-%d') for _ in range(len(names_authors))]\n",
    "death = []\n",
    "for i in range(len(names_authors)):\n",
    "    if i % 20 == 0:\n",
    "        death.append(faker_br.date_between(start_date='-100y', end_date='-18y').strftime('%Y-%m-%d'))\n",
    "    else:\n",
    "        death.append(None)\n",
    "\n",
    "\n",
    "data = [\n",
    "    (id, name, dt.strptime(b, \"%Y-%m-%d\").date(), dt.strptime(d, \"%Y-%m-%d\").date() if d else d)\n",
    "    for id, name, b, d in zip(id_authors, names_authors, birth, death)\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Inserindo\")\n",
    "insert_data(\"Autores\", data, ['id_autor', 'nome', 'data_nascimento', 'data_falecimento'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "author_df = pd.read_sql(\"SELECT * FROM autores\", conn)\n",
    "\n",
    "author_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7f229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "livros = pd.concat([pd.read_csv(path_join(LIVRO_PATH, file), dtype=str, sep=\",\") for file in os.listdir(LIVRO_PATH)], ignore_index=True)\n",
    "leng = livros.shape[0]\n",
    "for idx, row in livros.iterrows():\n",
    "    try:\n",
    "        biblios = random.randint(1, 5)\n",
    "        if idx % 10000 == 0: print(f\"Livros: {idx}/{leng}\")\n",
    "        for id_biblioteca in random.sample(biblio_ids, biblios):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Midia (tipo_midia, condicao, id_biblioteca)\n",
    "                VALUES (%s, %s, %s)\n",
    "                RETURNING id_midia;\n",
    "            \"\"\", ('livro', random.choice(['NOVO', 'SEMINOVO', 'DESGASTADO', 'DANIFICADO', 'ARQUIVADO']), id_biblioteca))\n",
    "\n",
    "            id_midia = cursor.fetchone()[0]\n",
    "\n",
    "            # 2. Inserir em Livros (com o mesmo id)\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Livros (id_livro, titulo, ISBN, numero_paginas, editora, data_publicacao)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s);\n",
    "            \"\"\", (id_midia, row['title'], str(row['isbn13']).replace('-', ''), row['pages'] if int(row['pages']) < 10000 else 4000, row['publisher'], faker_br.date_between(start_date='-100y', end_date='-1y')))\n",
    "\n",
    "            for ath in eval(row['authors']):\n",
    "                # 3. Inserir em Autor_Livro\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO Autorias (id_autor, id_midia)\n",
    "                    VALUES (%s, %s);\n",
    "                \"\"\", (ath, id_midia))\n",
    "    except Exception as e:\n",
    "        continue\n",
    "        \n",
    "\n",
    "conn.commit()\n",
    "del livros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_df.loc[df['nome'] == 'Yingxu Wang']['id_autor'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "revistas = pd.concat([pd.read_csv(path_join(REVISTA_PATH, file), dtype=str, sep=\",\") for file in os.listdir(REVISTA_PATH)], ignore_index=True)\n",
    "leng = revistas.shape[0]\n",
    "for idx, row in revistas.iterrows():\n",
    "    print(f\"Revistas: {idx}/{leng}\")\n",
    "    biblios = random.randint(1, 2)\n",
    "    try:\n",
    "        for id_biblioteca in random.sample(biblio_ids, biblios):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Midia (tipo_midia, condicao, id_biblioteca)\n",
    "                VALUES (%s, %s, %s)\n",
    "                RETURNING id_midia;\n",
    "            \"\"\", ('revista', random.choice(['NOVO', 'SEMINOVO', 'DESGASTADO', 'DANIFICADO', 'ARQUIVADO']), id_biblioteca))\n",
    "\n",
    "            id_midia = cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "            if not row['ISSN'] or row['ISSN'] == 'nan': \n",
    "                row['ISSN'] = faker_en.isbn13()\n",
    "            \n",
    "            \n",
    "            row['data_lancamento'] = faker_br.date_between(start_date='-60y', end_date='today').strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "            # 2. Inserir em Livros (com o mesmo id)\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Revistas (id_revista, titulo, ISSN, periodicidade, editora, data_publicacao)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s);\n",
    "            \"\"\", (id_midia, row['titulo'], row['ISSN'].replace('-', ''), row['periodicidade'], row['editora'], row['data_publicacao']))\n",
    "\n",
    "            for ath in row['autores'].split('|')[:2]:\n",
    "                id_autor = int(author_df.loc[author_df['nome'].str.strip() == ath.strip()]['id_autor'].values[0])\n",
    "\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO Autorias (id_autor, id_midia)\n",
    "                    VALUES (%s, %s);\n",
    "                \"\"\", (id_autor, id_midia))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "\n",
    "conn.commit()\n",
    "del revistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artigos = pd.concat([pd.read_csv(path_join(ARTIGO_PATH, file), dtype=str, sep=\",\") for file in os.listdir(ARTIGO_PATH)], ignore_index=True)\n",
    "# pd.read_csv('artigos/treated/arXiv_scientific_dataset.csv').isnull().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artigos = pd.concat([pd.read_csv(path_join(ARTIGO_PATH, file), dtype=str, sep=\",\") for file in os.listdir(ARTIGO_PATH)], ignore_index=True)\n",
    "# leng = artigos.shape[0]\n",
    "# for idx, row in artigos.iterrows():\n",
    "#     print(f\"Artigos: {idx}/{leng}\")\n",
    "#     biblios = random.randint(1, 5)\n",
    "#     try:\n",
    "#         for id_biblioteca in random.sample(biblio_ids, biblios):\n",
    "#             cursor.execute(\"\"\"\n",
    "#                 INSERT INTO Midia (tipo_midia, condicao, id_biblioteca)\n",
    "#                 VALUES (%s, %s, %s)\n",
    "#                 RETURNING id_midia;\n",
    "#             \"\"\", ('artigo', random.choice(['NOVO', 'SEMINOVO', 'DESGASTADO', 'DANIFICADO', 'ARQUIVADO']), id_biblioteca))\n",
    "\n",
    "#             id_midia = cursor.fetchone()[0]\n",
    "\n",
    "#             if not row['DOI']: \n",
    "#                 row['DOI'] = faker_en.isbn13()\n",
    "            \n",
    "#             if not row['data_publicacao'] or row['data_publicacao'] == 'nan': \n",
    "#                 row['data_publicacao'] = faker_br.date_between(start_date='-60y', end_date='today').strftime('%Y-%m-%d')\n",
    "\n",
    "#             # 2. Inserir em Livros (com o mesmo id)\n",
    "#             cursor.execute(\"\"\"\n",
    "#                 INSERT INTO artigos (id_artigo, titulo, DOI, publicadora, data_publicacao)\n",
    "#                 VALUES (%s, %s, %s, %s, %s);\n",
    "#             \"\"\", (id_midia, row['titulo'], row['DOI'].replace('-', '').replace('/', ''), row['publicadora'], row['data_publicacao']))\n",
    "\n",
    "#             for ath in row['autores'].split('|'):\n",
    "#                 id_autor = int(author_df.loc[df['nome'] == 'Yingxu Wang']['id_autor'].values[0])\n",
    "\n",
    "#                 cursor.execute(\"\"\"\n",
    "#                     INSERT INTO Autorias (id_autor, id_midia)\n",
    "#                     VALUES (%s, %s);\n",
    "#                 \"\"\", (id_autor, id_midia))\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         continue\n",
    "        \n",
    "\n",
    "# conn.commit()\n",
    "# del artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cdb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dvds = pd.concat([pd.read_csv(path_join(DVD_PATH, file), dtype=str, sep=\",\") for file in os.listdir(DVD_PATH)], ignore_index=True)\n",
    "\n",
    "# dvds.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    dbname=\"onixlibrary\",\n",
    "    user=\"super_user\",\n",
    "    password=\"carimboatrasado\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "dvds = pd.concat([pd.read_csv(path_join(DVD_PATH, file), dtype=str, sep=\",\") for file in os.listdir(DVD_PATH)], ignore_index=True)\n",
    "dvds['data_lancamento'] = dvds['data_lancamento'].astype(str)\n",
    "leng = dvds.shape[0]\n",
    "for idx, row in dvds.iterrows():\n",
    "    try:\n",
    "        print(f\"DVDs: {idx}/{leng}\")\n",
    "        biblios = random.randint(1, 5)\n",
    "        for id_biblioteca in random.sample(biblio_ids, biblios):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO Midia (tipo_midia, condicao, id_biblioteca)\n",
    "                VALUES (%s, %s, %s)\n",
    "                RETURNING id_midia;\n",
    "            \"\"\", ('dvd', random.choice(['NOVO', 'SEMINOVO', 'DESGASTADO', 'DANIFICADO', 'ARQUIVADO']), id_biblioteca))\n",
    "\n",
    "            id_midia = cursor.fetchone()[0]\n",
    "\n",
    "            try: \n",
    "                duracao = int(row['duracao']) \n",
    "                if duracao > 5000:\n",
    "                    duracao = 120\n",
    "            except ValueError:\n",
    "                duracao = 120\n",
    "\n",
    "            try:\n",
    "                # print(row['data_lancamento'])\n",
    "                lancamento = dt.strptime(row['data_lancamento'], \"%Y-%m-%d\").date()\n",
    "            except Exception as e:\n",
    "                lancamento= faker_br.date_between(start_date='-100y', end_date='today').strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "            # 2. Inserir em Livros (com o mesmo id)\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO dvds (id_dvd, titulo, ISAN, duracao, distribuidora, data_lancamento)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s);\n",
    "            \"\"\", (id_midia, row['titulo'], str(row['ISAN']).replace('-', ''), duracao , row['distribuidora'], lancamento))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "conn.commit()\n",
    "del dvds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a70f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Conexão com o banco de dados PostgreSQL\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"onixlibrary\",\n",
    "    user=\"super_user\",\n",
    "    password=\"carimboatrasado\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Parâmetros de quantidade\n",
    "NUM_EMPRESTIMOS = 5_954_982\n",
    "PENALIZACAO_PERCENTUAL = 0.1\n",
    "NUM_PENALIZACOES = int(NUM_EMPRESTIMOS * PENALIZACAO_PERCENTUAL)\n",
    "\n",
    "\n",
    "# IDs de mídia e usuários existentes\n",
    "cursor.execute(\"SELECT id_midia FROM Midia\")\n",
    "midias = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "cursor.execute(\"SELECT id_usuario FROM Usuario\")\n",
    "usuarios = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "emprestimos = []\n",
    "penalizacoes = []\n",
    "\n",
    "# Geração de empréstimos\n",
    "for _ in range(NUM_EMPRESTIMOS):\n",
    "    id_usuario = random.choice(usuarios)\n",
    "    id_midia = random.choice(midias)\n",
    "\n",
    "    data_emprestimo = faker_br.date_between(start_date=\"-20y\", end_date=\"today\")\n",
    "    data_prevista = data_emprestimo + timedelta(days=14)\n",
    "\n",
    "    devolvido = random.choices([True, False], weights=[90, 10])[0]\n",
    "    if devolvido:\n",
    "        atraso = random.randint(0, 10)\n",
    "        data_devolucao = data_prevista + timedelta(days=atraso)\n",
    "    else:\n",
    "        data_devolucao = None\n",
    "\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Emprestimo (data_emprestimo, data_devolucao_prevista, data_devolucao, id_midia, id_usuario)\n",
    "        VALUES (%s, %s, %s, %s, %s);\n",
    "    \"\"\", (data_emprestimo, data_prevista, data_devolucao, id_midia, id_usuario))\n",
    "\n",
    "\n",
    "# conn.commit()\n",
    "cursor.execute('SELECT id_emprestimo, id_usuario, data_devolucao FROM Emprestimo')\n",
    "\n",
    "# Geração de penalizações (10% aleatórios dos empréstimos)\n",
    "for id_emprestimo, id_usuario, devolucao in random.sample(cursor.fetchall(), NUM_PENALIZACOES):\n",
    "    try:\n",
    "        if id_usuario not in usuarios:\n",
    "            print(f\"Usuário {id_usuario} não encontrado.\")\n",
    "            continue\n",
    "        descricao = faker_br.sentence(nb_words=50)\n",
    "        fim_penalizacao = faker_br.date_between(start_date=devolucao if devolucao else \"-1y\", end_date=\"+90d\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO Penalizacao (descricao, Final_penalizacao, id_usuario, id_emprestimo)\n",
    "            VALUES (%s, %s, %s, %s);\n",
    "        \"\"\", (descricao, fim_penalizacao, id_usuario, id_emprestimo))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inserir penalização: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Commit e fechamento\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
